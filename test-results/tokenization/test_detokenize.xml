<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="0" skipped="0" tests="210" time="123.163" timestamp="2024-05-12T21:20:28.921192" hostname="nvidia-l4-gpu"><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-facebook/opt-125m-True-Hello here, this is a simple test]" time="0.435" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-facebook/opt-125m-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.351" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-facebook/opt-125m-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.441" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-facebook/opt-125m-False-Hello here, this is a simple test]" time="0.373" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-facebook/opt-125m-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.499" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-facebook/opt-125m-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.406" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-gpt2-True-Hello here, this is a simple test]" time="1.229" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-gpt2-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.316" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-gpt2-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.354" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-gpt2-False-Hello here, this is a simple test]" time="0.333" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-gpt2-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.438" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-gpt2-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.371" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigcode/tiny_starcoder_py-True-Hello here, this is a simple test]" time="1.099" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigcode/tiny_starcoder_py-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.306" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigcode/tiny_starcoder_py-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.313" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigcode/tiny_starcoder_py-False-Hello here, this is a simple test]" time="0.346" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigcode/tiny_starcoder_py-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.443" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigcode/tiny_starcoder_py-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.337" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/gpt-j-6b-True-Hello here, this is a simple test]" time="1.077" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/gpt-j-6b-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.369" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/gpt-j-6b-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.423" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/gpt-j-6b-False-Hello here, this is a simple test]" time="0.423" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/gpt-j-6b-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.531" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/gpt-j-6b-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.435" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/pythia-70m-True-Hello here, this is a simple test]" time="1.025" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/pythia-70m-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.305" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/pythia-70m-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.338" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/pythia-70m-False-Hello here, this is a simple test]" time="0.339" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/pythia-70m-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.443" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-EleutherAI/pythia-70m-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.367" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigscience/bloom-560m-True-Hello here, this is a simple test]" time="1.841" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigscience/bloom-560m-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="1.079" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigscience/bloom-560m-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="1.060" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigscience/bloom-560m-False-Hello here, this is a simple test]" time="1.721" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigscience/bloom-560m-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="2.572" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-bigscience/bloom-560m-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="1.229" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-mosaicml/mpt-7b-True-Hello here, this is a simple test]" time="0.819" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-mosaicml/mpt-7b-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.304" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-mosaicml/mpt-7b-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.328" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-mosaicml/mpt-7b-False-Hello here, this is a simple test]" time="0.339" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-mosaicml/mpt-7b-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.441" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-mosaicml/mpt-7b-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.344" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-tiiuae/falcon-7b-True-Hello here, this is a simple test]" time="0.810" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-tiiuae/falcon-7b-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.347" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-tiiuae/falcon-7b-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.405" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-tiiuae/falcon-7b-False-Hello here, this is a simple test]" time="0.405" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-tiiuae/falcon-7b-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.574" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-tiiuae/falcon-7b-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.423" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-meta-llama/Llama-2-7b-hf-True-Hello here, this is a simple test]" time="0.292" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-meta-llama/Llama-2-7b-hf-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.288" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-meta-llama/Llama-2-7b-hf-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.321" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-meta-llama/Llama-2-7b-hf-False-Hello here, this is a simple test]" time="0.331" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-meta-llama/Llama-2-7b-hf-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.374" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-meta-llama/Llama-2-7b-hf-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.331" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-codellama/CodeLlama-7b-hf-True-Hello here, this is a simple test]" time="0.794" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-codellama/CodeLlama-7b-hf-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.295" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-codellama/CodeLlama-7b-hf-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.331" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-codellama/CodeLlama-7b-hf-False-Hello here, this is a simple test]" time="0.422" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-codellama/CodeLlama-7b-hf-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.396" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[True-codellama/CodeLlama-7b-hf-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.339" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-facebook/opt-125m-True-Hello here, this is a simple test]" time="0.335" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-facebook/opt-125m-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.343" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-facebook/opt-125m-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.392" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-facebook/opt-125m-False-Hello here, this is a simple test]" time="0.365" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-facebook/opt-125m-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.535" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-facebook/opt-125m-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.397" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-gpt2-True-Hello here, this is a simple test]" time="0.309" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-gpt2-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.303" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-gpt2-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.351" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-gpt2-False-Hello here, this is a simple test]" time="0.340" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-gpt2-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.433" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-gpt2-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.705" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigcode/tiny_starcoder_py-True-Hello here, this is a simple test]" time="0.312" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigcode/tiny_starcoder_py-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.296" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigcode/tiny_starcoder_py-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.316" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigcode/tiny_starcoder_py-False-Hello here, this is a simple test]" time="0.348" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigcode/tiny_starcoder_py-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.447" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigcode/tiny_starcoder_py-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.342" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/gpt-j-6b-True-Hello here, this is a simple test]" time="0.355" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/gpt-j-6b-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.362" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/gpt-j-6b-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.428" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/gpt-j-6b-False-Hello here, this is a simple test]" time="0.395" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/gpt-j-6b-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.522" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/gpt-j-6b-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.420" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/pythia-70m-True-Hello here, this is a simple test]" time="0.303" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/pythia-70m-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.291" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/pythia-70m-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.325" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/pythia-70m-False-Hello here, this is a simple test]" time="0.339" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/pythia-70m-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.442" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-EleutherAI/pythia-70m-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.342" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigscience/bloom-560m-True-Hello here, this is a simple test]" time="1.013" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigscience/bloom-560m-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="1.016" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigscience/bloom-560m-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="1.012" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigscience/bloom-560m-False-Hello here, this is a simple test]" time="1.418" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigscience/bloom-560m-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="2.571" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-bigscience/bloom-560m-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="1.215" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-mosaicml/mpt-7b-True-Hello here, this is a simple test]" time="0.299" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-mosaicml/mpt-7b-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.370" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-mosaicml/mpt-7b-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.331" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-mosaicml/mpt-7b-False-Hello here, this is a simple test]" time="0.335" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-mosaicml/mpt-7b-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.437" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-mosaicml/mpt-7b-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.348" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-tiiuae/falcon-7b-True-Hello here, this is a simple test]" time="0.343" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-tiiuae/falcon-7b-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.339" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-tiiuae/falcon-7b-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.398" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-tiiuae/falcon-7b-False-Hello here, this is a simple test]" time="0.421" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-tiiuae/falcon-7b-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.553" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-tiiuae/falcon-7b-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.414" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-meta-llama/Llama-2-7b-hf-True-Hello here, this is a simple test]" time="0.293" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-meta-llama/Llama-2-7b-hf-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.288" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-meta-llama/Llama-2-7b-hf-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.332" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-meta-llama/Llama-2-7b-hf-False-Hello here, this is a simple test]" time="0.314" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-meta-llama/Llama-2-7b-hf-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.386" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-meta-llama/Llama-2-7b-hf-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.334" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-codellama/CodeLlama-7b-hf-True-Hello here, this is a simple test]" time="0.288" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-codellama/CodeLlama-7b-hf-True-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.285" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-codellama/CodeLlama-7b-hf-True-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.318" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-codellama/CodeLlama-7b-hf-False-Hello here, this is a simple test]" time="0.310" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-codellama/CodeLlama-7b-hf-False-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.378" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_streaming[False-codellama/CodeLlama-7b-hf-False-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.343" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-facebook/opt-125m-Hello here, this is a simple test]" time="0.530" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-facebook/opt-125m-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.518" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-facebook/opt-125m-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.519" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-gpt2-Hello here, this is a simple test]" time="0.453" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-gpt2-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.438" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-gpt2-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.454" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-bigcode/tiny_starcoder_py-Hello here, this is a simple test]" time="0.449" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-bigcode/tiny_starcoder_py-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.445" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-bigcode/tiny_starcoder_py-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.442" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-EleutherAI/gpt-j-6b-Hello here, this is a simple test]" time="0.589" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-EleutherAI/gpt-j-6b-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.587" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-EleutherAI/gpt-j-6b-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.575" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-EleutherAI/pythia-70m-Hello here, this is a simple test]" time="0.450" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-EleutherAI/pythia-70m-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.458" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-EleutherAI/pythia-70m-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.453" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-bigscience/bloom-560m-Hello here, this is a simple test]" time="1.798" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-bigscience/bloom-560m-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="1.836" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-bigscience/bloom-560m-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="1.783" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-mosaicml/mpt-7b-Hello here, this is a simple test]" time="0.453" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-mosaicml/mpt-7b-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.463" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-mosaicml/mpt-7b-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.451" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-tiiuae/falcon-7b-Hello here, this is a simple test]" time="0.542" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-tiiuae/falcon-7b-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.533" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-tiiuae/falcon-7b-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.530" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-meta-llama/Llama-2-7b-hf-Hello here, this is a simple test]" time="0.448" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-meta-llama/Llama-2-7b-hf-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.442" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-meta-llama/Llama-2-7b-hf-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.438" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-codellama/CodeLlama-7b-hf-Hello here, this is a simple test]" time="0.455" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-codellama/CodeLlama-7b-hf-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.432" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[True-codellama/CodeLlama-7b-hf-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.442" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-facebook/opt-125m-Hello here, this is a simple test]" time="0.516" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-facebook/opt-125m-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.520" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-facebook/opt-125m-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.523" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-gpt2-Hello here, this is a simple test]" time="0.455" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-gpt2-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.447" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-gpt2-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.456" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-bigcode/tiny_starcoder_py-Hello here, this is a simple test]" time="0.449" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-bigcode/tiny_starcoder_py-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.455" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-bigcode/tiny_starcoder_py-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.455" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-EleutherAI/gpt-j-6b-Hello here, this is a simple test]" time="0.581" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-EleutherAI/gpt-j-6b-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.581" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-EleutherAI/gpt-j-6b-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.581" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-EleutherAI/pythia-70m-Hello here, this is a simple test]" time="0.464" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-EleutherAI/pythia-70m-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.466" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-EleutherAI/pythia-70m-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.451" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-bigscience/bloom-560m-Hello here, this is a simple test]" time="1.801" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-bigscience/bloom-560m-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="1.803" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-bigscience/bloom-560m-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="1.784" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-mosaicml/mpt-7b-Hello here, this is a simple test]" time="0.464" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-mosaicml/mpt-7b-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.468" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-mosaicml/mpt-7b-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.475" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-tiiuae/falcon-7b-Hello here, this is a simple test]" time="0.545" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-tiiuae/falcon-7b-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.541" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-tiiuae/falcon-7b-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.531" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-meta-llama/Llama-2-7b-hf-Hello here, this is a simple test]" time="0.437" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-meta-llama/Llama-2-7b-hf-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.440" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-meta-llama/Llama-2-7b-hf-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.441" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-codellama/CodeLlama-7b-hf-Hello here, this is a simple test]" time="0.439" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-codellama/CodeLlama-7b-hf-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.437" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_sequence_logprobs[False-codellama/CodeLlama-7b-hf-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.456" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-facebook/opt-125m-Hello here, this is a simple test]" time="0.508" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-facebook/opt-125m-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.523" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-facebook/opt-125m-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.684" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-gpt2-Hello here, this is a simple test]" time="0.550" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-gpt2-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.490" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-gpt2-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.458" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-bigcode/tiny_starcoder_py-Hello here, this is a simple test]" time="0.463" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-bigcode/tiny_starcoder_py-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.462" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-bigcode/tiny_starcoder_py-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.457" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-EleutherAI/gpt-j-6b-Hello here, this is a simple test]" time="0.586" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-EleutherAI/gpt-j-6b-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.585" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-EleutherAI/gpt-j-6b-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.617" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-EleutherAI/pythia-70m-Hello here, this is a simple test]" time="0.462" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-EleutherAI/pythia-70m-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.453" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-EleutherAI/pythia-70m-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.461" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-bigscience/bloom-560m-Hello here, this is a simple test]" time="1.782" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-bigscience/bloom-560m-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="1.765" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-bigscience/bloom-560m-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="1.772" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-mosaicml/mpt-7b-Hello here, this is a simple test]" time="0.454" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-mosaicml/mpt-7b-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.467" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-mosaicml/mpt-7b-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.478" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-tiiuae/falcon-7b-Hello here, this is a simple test]" time="0.519" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-tiiuae/falcon-7b-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.538" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-tiiuae/falcon-7b-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.527" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-meta-llama/Llama-2-7b-hf-Hello here, this is a simple test]" time="0.429" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-meta-llama/Llama-2-7b-hf-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.433" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-meta-llama/Llama-2-7b-hf-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.436" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-codellama/CodeLlama-7b-hf-Hello here, this is a simple test]" time="0.436" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-codellama/CodeLlama-7b-hf-vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving]" time="0.441" /><testcase classname="tests.tokenization.test_detokenize" name="test_decode_prompt_logprobs[True-codellama/CodeLlama-7b-hf-\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5]" time="0.427" /></testsuite></testsuites>