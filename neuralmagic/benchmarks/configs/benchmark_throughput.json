{
	"configs": [
		{
			"description": "Benchmark vllm engine throughput - with dataset",
			"dataset_download_cmds": [
				"wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json"
			],
			"models": [
				"facebook/opt-125m",
				"PY007/TinyLlama-1.1B-step-50K-105b",
				"mistralai/Mistral-7B-Instruct-v0.2",
				"NousResearch/Llama-2-7b-chat-hf",
				"mistralai/Mixtral-8x7B-Instruct-v0.1",
				"codellama/CodeLlama-34b-hf",
				"NousResearch/Llama-2-70b-chat-hf"
			],
			"script_name": "benchmark_throughput.py",
			"script_args": {
				"backend": [
					"vllm"
				],
				"dataset": [
					"ShareGPT_V3_unfiltered_cleaned_split.json"
				],
				"output-len": [
					128
				],
				"tensor-parallel-size": [
					1
				],
				"n": [
					1
				],
				"num-prompts": [
					1000
				],
				"seed": [
					0
				],
				"dtype": [
					"auto"
				]
			}
		},
		{
			"description": "Benchmark vllm engine prefill throughput - synthetic",
			"dataset_download_cmds": [],
			"models": [
				"facebook/opt-125m",
				"PY007/TinyLlama-1.1B-step-50K-105b",
				"mistralai/Mistral-7B-Instruct-v0.2",
				"NousResearch/Llama-2-7b-chat-hf",
				"mistralai/Mixtral-8x7B-Instruct-v0.1",
				"codellama/CodeLlama-34b-hf",
				"NousResearch/Llama-2-70b-chat-hf"
			],
			"script_name": "benchmark_throughput.py",
			"script_args": {
				"backend": [
					"vllm"
				],
				"input-len": [
					1,
					16,
					32,
					64,
					128,
					256,
					512,
					1024
				],
				"output-len": [
					1
				],
				"tensor-parallel-size": [
					1
				],
				"n": [
					1
				],
				"num-prompts": [
					1
				],
				"seed": [
					0
				],
				"dtype": [
					"auto"
				]
			}
		},
		{
			"description": "Benchmark vllm engine decode throughput - synthetic",
			"dataset_download_cmds": [],
			"models": [
				"facebook/opt-125m",
				"PY007/TinyLlama-1.1B-step-50K-105b",
				"mistralai/Mistral-7B-Instruct-v0.2",
				"NousResearch/Llama-2-7b-chat-hf",
				"mistralai/Mixtral-8x7B-Instruct-v0.1",
				"codellama/CodeLlama-34b-hf",
				"NousResearch/Llama-2-70b-chat-hf"
			],
			"script_name": "benchmark_throughput.py",
			"script_args": {
				"backend": [
					"vllm"
				],
				"input-len": [
					2
				],
				"output-len": [
					128
				],
				"tensor-parallel-size": [
					1
				],
				"n": [
					1
				],
				"num-prompts": [
					1,
					4,
					8,
					16,
					32,
					64
				],
				"seed": [
					0
				],
				"dtype": [
					"auto"
				]
			}
		}
	]
}