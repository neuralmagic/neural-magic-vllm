{
   "configs" :[ {
    	"description" : "Benchmark vllm engine throughput - with dataset",
    	"dataset_download_cmds" : ["wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json"],


	"models" : ["facebook/opt-125m",
	 	"PY007/TinyLlama-1.1B-step-50K-105b",
		"mistralai/Mistral-7B-Instruct-v0.2",
		"NousResearch/Llama-2-7b-chat-hf",
		"mistralai/Mixtral-8x7B-Instruct-v0.1",
		"codellama/CodeLlama-34b-hf",
		"NousResearch/Llama-2-70b-chat-hf"
		],
    	"script_name" : "benchmark_throughput.py",
    
    	"script_args" : {
            "backend" : ["vllm"],
    		"dataset" : ["ShareGPT_V3_unfiltered_cleaned_split.json"],
            "output-len" : [128],
            "tensor-parallel-size" : [1],
            "n" : [1],
            "num-prompts" : [1000],
            "seed" : [0],
            "dtype": ["auto"]
    	}
    },
    {
    	"description" : "Benchmark vllm engine throughput - synthetic",

    	"dataset_download_cmds" : [],
	"models" : ["facebook/opt-125m",
	 	"PY007/TinyLlama-1.1B-step-50K-105b",
		"mistralai/Mistral-7B-Instruct-v0.2",
		"NousResearch/Llama-2-7b-chat-hf",
		"mistralai/Mixtral-8x7B-Instruct-v0.1",
		"codellama/CodeLlama-34b-hf",
		"NousResearch/Llama-2-70b-chat-hf"
		],
    	"script_name" : "benchmark_throughput.py",
    
    	"script_args" : {
            "backend" : ["vllm"],
            "input-len" : [1, 16, 32, 64, 128, 256, 512, 1024],
            "output-len" : [128],
            "tensor-parallel-size" : [1],
            "n" : [1],
            "num-prompts" : [1000],
            "seed" : [0],
            "dtype": ["auto"]
    	}
    }]
}