{
	"configs": [
		{
			"description": "VLLM Engine decode throughput - Dense (synthetic)",
			"models": [
				"teknium/OpenHermes-2.5-Mistral-7B",
				"neuralmagic/OpenHermes-2.5-Mistral-7B-marlin",
				"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ",
				"NousResearch/Llama-2-7b-chat-hf"
			],
			"max_model_lens": [
				4096
			],
			"script_name": "benchmark_throughput",
			"script_args": {
				"input-len": [
					2
				],
				"output-len": [
					128
				],
				"num-prompts": [
					1,
					4,
					8,
					16,
					32,
					64
				],
				"use-all-available-gpus_": []
			}
		},
		{
			"description": "VLLM Engine decode throughput - Sparse (synthetic)",
			"models": [
				"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned50"
			],
			"max_model_lens": [
				4096
			],
			"script_name": "benchmark_throughput",
			"script_args": {
				"input-len": [
					2
				],
				"output-len": [
					128
				],
				"num-prompts": [
					1,
					4,
					8,
					16,
					32,
					64
				],
				"sparsity": [
					"sparse_w16a16"
				],
				"use-all-available-gpus_": []
			}
		},
		{
			"description": "VLLM Engine decode throughput - 2:4 Sparse (synthetic)",
			"models": [
				"neuralmagic/OpenHermes-2.5-Mistral-7B-pruned2.4"
			],
			"max_model_lens": [
				4096
			],
			"script_name": "benchmark_throughput",
			"script_args": {
				"input-len": [
					2
				],
				"output-len": [
					128
				],
				"num-prompts": [
					1,
					4,
					8,
					16,
					32,
					64
				],
				"sparsity": [
					"semi_structured_sparse_w16a16"
				],
				"use-all-available-gpus_": []
			}
		}
	]
}