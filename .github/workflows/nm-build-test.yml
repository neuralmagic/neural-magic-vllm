name: nm build-test
on:

  # makes workflow reusable
  workflow_call:
    inputs:
      wf_category:
        description: "workflow category: REMOTE, NIGHTLY, RELEASE"
        type: string
        default: "REMOTE"
      push_to_pypi:
        description: "When set to true, built wheels and tar.gz will be pushed to neuralmagic pypi if all tests pass"
        type: boolean
        default: false
      python:
        description: "python version, e.g. 3.10.12"
        type: string
        required: true

      # build related parameters
      build_label:
        description: "requested runner label (specifies instance)"
        type: string
        default: gcp-k8s-build
      build_timeout:
        description: "time limit for build in minutes"
        type: string
        default: "120"
      Gi_per_thread:
        description: 'requested GiB to reserve per thread'
        type: string
        default: "1"
      nvcc_threads:
        description: "number of threads nvcc build threads"
        type: string
        default: "8"

      # test related parameters

      # stringified Json array of maps
      # each map has a "python", "gha label", "test skip env vars" e.g.
      # [
      #     {'python':'3.8.17','label':'gcp-k8s-l4-solo','test':'neuralmagic/tests/test_skip_env_vars/smoke.txt'},
      #     ...
      # ]
      test_configs:
        description: "python, label, skip envs"
        type: string
        required: true

      test_timeout:
        description: "time limit for test run in minutes"
        type: string
        required: true
      gitref:
        description: "git commit hash or branch name"
        type: string
        required: true

      # benchmark related parameters
      benchmark_label:
        description: "requested benchmark label (specifies instance)"
        type: string
        default: ""
      benchmark_config_list_file:
        description: "benchmark configs file, e.g. 'nm_benchmark_nightly_configs_list.txt'"
        type: string
        required: true
      benchmark_timeout:
        description: "time limit for benchmarking"
        type: string
        default: "720"
      push_benchmark_results_to_gh_pages:
        description: "when set to true, the workflow pushes all benchmarking results to gh-pages UI"
        type: boolean
        default: false

      # lm-eval related parameters
      lm_eval_label:
        description: "requested runner label (specifies instance)"
        type: string
        default: ""
      lm_eval_timeout:
        description: "time limit for lm_eval in minutes"
        type: string
        default: "60"
      lm_eval_configuration:
        description: "configuration for lm-eval test (see .github/lm-eval-configs)"
        type: string
        default: ""

jobs:

    JSON-VALIDATE:
        runs-on: gcp-k8s-util
        strategy:
            matrix:
                test_config: ${{ fromJson(inputs.test_configs) }}
        steps:
            - name: validate test config
              run: |
                echo "python: ${{ matrix.test_config.python }}"
                echo "label: ${{ matrix.test_config.label }}"
                echo "tests: ${{ matrix.test_config.test }}"

    BUILD:
        needs: [JSON-VALIDATE]
        uses: ./.github/workflows/nm-build.yml
        with:
            wf_category: ${{ inputs.wf_category }}
            build_label: ${{ inputs.build_label }}
            timeout: ${{ inputs.build_timeout }}
            gitref: ${{ github.ref }}
            Gi_per_thread: ${{ inputs.Gi_per_thread }}
            nvcc_threads: ${{ inputs.nvcc_threads }}
            python: ${{ inputs.python }}
        secrets: inherit

    TEST:
        needs: [BUILD]
        if: success()
        strategy:
            fail-fast: false
            matrix:
                test_config: ${{ fromJson(inputs.test_configs) }}
        uses: ./.github/workflows/nm-test.yml
        with:
            test_label: ${{ matrix.test_config.label }}
            timeout: ${{ inputs.test_timeout }}
            gitref: ${{ github.ref }}
            python: ${{ matrix.test_config.python }}
            whl: ${{ needs.BUILD.outputs.whl }}
            test_skip_env_vars: ${{ matrix.test_config.test }}
        secrets: inherit

    BENCHMARK:
        needs: [BUILD]
        if: success()
        uses: ./.github/workflows/nm-benchmark.yml
        with:
            label: ${{ inputs.benchmark_label }}
            benchmark_config_list_file: ${{ inputs.benchmark_config_list_file }}
            timeout: ${{ inputs.benchmark_timeout }}
            gitref: ${{ github.ref }}
            python: ${{ inputs.python }}
            whl: ${{ needs.BUILD.outputs.whl }}
            # Always push if it is a scheduled job
            push_benchmark_results_to_gh_pages: "${{ github.event_name == 'schedule' || inputs.push_benchmark_results_to_gh_pages }}"
        secrets: inherit

    LM-EVAL:
      needs: [BUILD]
      uses: ./.github/workflows/nm-lm-eval.yml
      with:
        label: ${{ inputs.lm_eval_label }}
        timeout: ${{ inputs.lm_eval_timeout }}
        gitref: ${{ inputs.gitref }}
        python: ${{ inputs.python }}
        whl: ${{ needs.BUILD.outputs.whl }}
        lm_eval_configuration: ${{ inputs.lm_eval_configuration }}
      secrets: inherit

    # uploading is only available when using GCP autoscaling group
    UPLOAD:
        needs: [TEST, BENCHMARK, LM-EVAL]
        if: ${{ inputs.push_to_pypi }}
        uses: ./.github/workflows/nm-upload-assets-to-gcp.yml
        with:
            label: gcp-k8s-util
            timeout: ${{ inputs.build_timeout }}
            gitref: ${{ github.ref }}
        secrets: inherit

    # update docker
    DOCKER:
        needs: [BUILD, UPLOAD]
        if: ${{ inputs.push_to_pypi }}
        uses: ./.github/workflows/publish-docker.yml
        with:
            push_to_repository: ${{ inputs.push_to_pypi }}
            gitref: ${{ inputs.gitref }}
            wf_category: ${{ inputs.wf_category }}
            wheel: ${{ needs.BUILD.outputs.whl }}
        secrets: inherit
