name: nm build-test
on:
  # makes workflow reusable
  workflow_call:
    inputs:
      wf_category:
        description: "categories: REMOTE, NIGHTLY, WEEKLY, RELEASE"
        type: string
        default: "REMOTE"
      python:
        description: "python version, e.g. 3.10.12"
        type: string
        required: true
      # build related parameters
      build_label:
        description: "requested runner label (specifies instance)"
        type: string
        default: gcp-k8s-build
      build_timeout:
        description: "time limit for build in minutes "
        type: string
        default: "120"
      Gi_per_thread:
        description: 'requested GiB to reserve per thread'
        type: string
        default: "1"
      nvcc_threads:
        description: "number of threads nvcc build threads"
        type: string
        default: "8"
      # test related parameters
      test_label_solo:
        description: "requested runner label (specifies instance)"
        type: string
        required: true
      test_label_multi:
        description: "requested runner label (specifies instance)"
        type: string
        required: true
      test_timeout:
        description: "time limit for test run in minutes "
        type: string
        required: true
      gitref:
        description: "git commit hash or branch name"
        type: string
        required: true
      test_skip_env_vars:
        description: 'file with list of env vars controlling which tests to run'
        type: string
        required: true
      # benchmark related parameters
      benchmark_label:
        description: "requested benchmark label (specifies instance)"
        type: string
        default: ""
      benchmark_config_list_file:
        description: "benchmark configs file, e.g. 'nm_benchmark_nightly_configs_list.txt'"
        type: string
        required: true
      benchmark_timeout:
        description: "time limit for benchmarking"
        type: string
        default: "720"
      push_benchmark_results_to_gh_pages:
        description: "When set to true, the workflow pushes all benchmarking results to gh-pages UI"
        type: string
        default: "false"
      # lm-eval related parameters
      lm_eval_label:
        description: "requested runner label (specifies instance)"
        type: string
        default: ""
      lm_eval_timeout:
        description: "time limit for lm_eval in minutes"
        type: string
        default: "60"
      lm_eval_configuration:
        description: "configuration for lm-eval test (see neuralmagic/lm-eval)"
        type: string
        default: "" 

jobs:

    BUILD:
        uses: ./.github/workflows/nm-build.yml
        with:
            wf_category: ${{ inputs.wf_category }}
            build_label: ${{ inputs.build_label }}
            timeout: ${{ inputs.build_timeout }}
            gitref: ${{ github.ref }}
            Gi_per_thread: ${{ inputs.Gi_per_thread }}
            nvcc_threads: ${{ inputs.nvcc_threads }}
            python: ${{ inputs.python }}
        secrets: inherit

    TEST-SOLO:
        needs: [BUILD]
        if: success()
        uses: ./.github/workflows/nm-test.yml
        with:
            test_label: ${{ inputs.test_label_solo }}
            timeout: ${{ inputs.test_timeout }}
            gitref: ${{ github.ref }}
            python: ${{ inputs.python }}
            whl: ${{ needs.BUILD.outputs.whl }}
            test_skip_env_vars: ${{ inputs.test_skip_env_vars }}
        secrets: inherit

    # TODO: re-enable
    # TEST-MULTI:
    #     needs: [BUILD]
    #     if: success() && contains(fromJSON('["NIGHTLY", "WEEKLY", "RELEASE"]'), inputs.wf_category)
    #     uses: ./.github/workflows/nm-test.yml
    #     with:
    #         test_label: ${{ inputs.test_label_multi }}
    #         timeout: ${{ inputs.test_timeout }}
    #         gitref: ${{ github.ref }}
    #         python: ${{ inputs.python }}
    #         whl: ${{ needs.BUILD.outputs.whl }}
    #         test_skip_env_vars: ${{ inputs.test_skip_env_vars }}
    #     secrets: inherit

    UPLOAD:
        needs: [TEST-SOLO]
        if: contains(fromJSON('["NIGHTLY", "WEEKLY", "RELEASE"]'), inputs.wf_category)
        uses: ./.github/workflows/nm-upload-assets-to-gcp.yml
        with:
            label: ${{ inputs.build_label }}
            timeout: ${{ inputs.build_timeout }}
            gitref: ${{ github.ref }}
            python: ${{ inputs.python }}
        secrets: inherit

    BENCHMARK:
        needs: [BUILD]
        if: success()
        uses: ./.github/workflows/nm-benchmark.yml
        with:
            label: ${{ inputs.benchmark_label }}
            benchmark_config_list_file: ${{ inputs.benchmark_config_list_file }}
            timeout: ${{ inputs.benchmark_timeout }}
            gitref: ${{ github.ref }}
            python: ${{ inputs.python }}
            whl: ${{ needs.BUILD.outputs.whl }}
            # Always push if it is a scheduled job
            push_benchmark_results_to_gh_pages: "${{ github.event_name == 'schedule' || inputs.push_benchmark_results_to_gh_pages }}"
        secrets: inherit

    LM-EVAL-SOLO:
      needs: [BUILD]
      uses: ./.github/workflows/nm-lm-eval.yml
      with:
        label: ${{ inputs.lm_eval_label }}
        timeout: ${{ inputs.lm_eval_timeout }}
        gitref: ${{ inputs.gitref }}
        python: ${{ inputs.python }}
        whl: ${{ needs.BUILD.outputs.whl }}
        lm_eval_configuration: ${{ inputs.lm_eval_configuration }}
      secrets: inherit
