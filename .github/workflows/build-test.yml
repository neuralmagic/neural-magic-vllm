name: build-test
on:
  # makes workflow reusable
  workflow_call:
    inputs:
      wf_category:
        description: "categories: REMOTE, NIGHTLY, WEEKLY, RELEASE"
        type: string
        default: "REMOTE"
      python:
        description: "python version, e.g. 3.10.12"
        type: string
        required: true
      # build related parameters
      build_label:
        description: "requested runner label (specifies instance)"
        type: string
        default: "gcp-build-static"
      build_timeout:
        description: "time limit for build in minutes "
        type: string
        default: "60"
      Gi_per_thread:
        description: 'requested GiB to reserve per thread'
        type: string
        default: "1"
      nvcc_threads:
        description: "number of threads nvcc build threads"
        type: string
        default: "4"
      # test related parameters
      test_label_solo:
        description: "requested runner label (specifies instance)"
        type: string
        required: true
      test_label_multi:
        description: "requested runner label (specifies instance)"
        type: string
        required: true
      test_timeout:
        description: "time limit for test run in minutes "
        type: string
        required: true
      gitref:
        description: "git commit hash or branch name"
        type: string
        required: true
      test_skip_list:
        description: 'file containing tests to skip'
        type: string
        required: true
      # benchmark related parameters
      benchmark_label:
        description: "requested benchmark label (specifies instance)"
        type: string
        default: ""
      benchmark_config_list_file:
        description: "benchmark configs file, e.g. 'nm_benchmark_nightly_configs_list.txt'"
        type: string
        required: true
      benchmark_timeout:
        description: "time limit for benchmarking"
        type: string
        default: "720"
      push_benchmark_results_to_gh_pages:
        description: "When set to true, the workflow pushes all benchmarking results to gh-pages UI"
        type: string
        default: "false"

  # makes workflow manually callable
  workflow_dispatch:
    inputs:
      build_label:
        description: "requested runner label (specifies instance)"
        type: string
        required: true
      build_timeout:
        description: "time limit for build in minutes "
        type: string
        required: true
      test_label_solo:
        description: "requested runner label (specifies instance)"
        type: string
        required: true
      test_label_multi:
        description: "requested runner label (specifies instance)"
        type: string
        required: true
      test_timeout:
        description: "time limit for test run in minutes "
        type: string
        required: true
      gitref:
        description: "git commit hash or branch name"
        type: string
        required: true
      Gi_per_thread:
        description: 'requested GiB to reserve per thread'
        type: string
        required: true
      nvcc_threads:
        description: "number of threads nvcc build threads"
        type: string
        required: true
      python:
        description: "python version, e.g. 3.10.12"
        type: string
        required: true
      test_skip_list:
        description: 'file containing tests to skip'
        type: string
        required: true

jobs:

    BUILD:
        uses: ./.github/workflows/build.yml
        with:
            build_label: ${{ inputs.build_label }}
            timeout: ${{ inputs.build_timeout }}
            gitref: ${{ github.ref }}
            Gi_per_thread: ${{ inputs.Gi_per_thread }}
            nvcc_threads: ${{ inputs.nvcc_threads }}
            python: ${{ inputs.python }}
        secrets: inherit

    TEST-SOLO:
        needs: [BUILD]
        if: success()
        uses: ./.github/workflows/test.yml
        with:
            test_label: ${{ inputs.test_label_solo }}
            timeout: ${{ inputs.test_timeout }}
            gitref: ${{ github.ref }}
            python: ${{ inputs.python }}
            whl: ${{ needs.BUILD.outputs.whl }}
            test_skip_list: ${{ inputs.test_skip_list }}
        secrets: inherit

    TEST-MULTI:
        needs: [BUILD]
        if: success() && contains(fromJSON('["NIGHTLY", "RELEASE"]'), inputs.wf_category)
        uses: ./.github/workflows/test.yml
        with:
            test_label: ${{ inputs.test_label_multi }}
            timeout: ${{ inputs.test_timeout }}
            gitref: ${{ github.ref }}
            python: ${{ inputs.python }}
            whl: ${{ needs.BUILD.outputs.whl }}
            test_skip_list: ${{ inputs.test_skip_list }}
        secrets: inherit

    PUBLISH:
        needs: [TEST-SOLO, TEST-MULTI]
        uses: ./.github/workflows/nm-publish.yml
        with:
            label: ${{ inputs.build_label }}
            timeout: ${{ inputs.build_timeout }}
            gitref: ${{ github.ref }}
            python: ${{ inputs.python }}
            whl: ${{ needs.BUILD.outputs.whl }}
            tarfile: ${{ needs.BUILD.outputs.tarfile }}
        secrets: inherit

    BENCHMARK:
        needs: [BUILD]
        if: success()
        uses: ./.github/workflows/nm-benchmark.yml
        with:
            label: ${{ inputs.test_label_solo }}
            benchmark_config_list_file: ${{ inputs.benchmark_config_list_file }}
            timeout: ${{ inputs.benchmark_timeout }}
            gitref: ${{ github.ref }}
            python: ${{ inputs.python }}
            whl: ${{ needs.BUILD.outputs.whl }}
            # Always push if it is a scheduled job
            push_benchmark_results_to_gh_pages: "${{ github.event_name == 'schedule' || inputs.push_benchmark_results_to_gh_pages }}"
        secrets: inherit

    TEST-ACCURACY-SMOKE:
      needs: [BUILD]
      if: inputs.wf_category == 'NIGHTLY'
      uses: ./.github/workflows/nm-lm-eval-smoke.yml
      with:
        label: ${{ inputs.test_label_solo }}
        timeout: ${{ inputs.benchmark_timeout }}
        gitref: ${{ inputs.gitref }}
        Gi_per_thread: ${{ inputs.Gi_per_thread }}
        nvcc_threads: ${{ inputs.nvcc_threads }}
        python: ${{ inputs.python }}
        whl: ${{ needs.BUILD.outputs.whl }}
      secrets: inherit

    TEST-ACCURACY-FULL:
      needs: [BUILD]
      if: ${{ inputs.wf_category == 'WEEKLY' || inputs.wf_category == 'RELEASE' }}
      uses: ./.github/workflows/nm-lm-eval-accuracy.yml
      with:
        label: ${{ inputs.test_label_multi }}
        timeout: ${{ inputs.benchmark_timeout }}
        gitref: ${{ inputs.gitref }}
        Gi_per_thread: ${{ inputs.Gi_per_thread }}
        nvcc_threads: ${{ inputs.nvcc_threads }}
        python: ${{ inputs.python }}
        whl: ${{ needs.BUILD.outputs.whl }}
      secrets: inherit
