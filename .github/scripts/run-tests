#!/bin/bash -e

# simple helper script to manage concurrency while running tests

usage() {
    echo "Usage: ${0} <options>"
    echo
    echo "  -s    - src directory, i.e. location of package *.py files."
    echo "  -t    - test directory, i.e. location of *.py test files. (default 'tests/')"
    echo "  -r    - desired results base directory. xml results will mirror provided tests directory structure. (default 'test-results/')"
    echo "  -f    - file with test skip list, e.g. ' neuralmagic/tests/skip-for-remote-push.txt'. (default is to run all found tests)"
    echo "  -h    - this list of options"
    echo
    echo "note: all paths are relative to 'nm-vllm' root"
    echo
    exit 1
}

SRC_DIR=
TEST_DIR=tests
RESULTS_DIR=test-results
SKIP_LIST=

while getopts "hs:t:r:f:" OPT; do
    case "${OPT}" in
	h)
	    usage
	    ;;
    s)
        SRC_DIR="${OPTARG}"
        ;;
	t)
	    TEST_DIR="${OPTARG}"
	    ;;
    r)
        RESULTS_DIR="${OPTARG}"
        ;;
    f)
        SKIP_LIST="${OPTARG}"
        ;;
    esac
done

# check if variables are valid
if [ -z "${SRC_DIR}" ]; then
    echo "please set desired source directory"
    usage
fi

if [ -z "${RESULTS_DIR}" ]; then
    echo "please set desired results base directory"
    usage
fi

if [ -z "${TEST_DIR}" ]; then
    echo "please set test directory"
    usage
fi

if [ ! -d "${TEST_DIR}" ]; then
    echo "specified test directory, '${TEST_DIR}' does not exist ..."
    usage
fi

# find tests
TESTS_DOT_PY=$(find ${TEST_DIR} -type f -name "test*.py")
TESTS_FOUND=(${TESTS_DOT_PY})

echo "found:"
for FOUND in "${TESTS_FOUND[@]}"; do
    echo "${FOUND}"
done

# build the skip list from provided file
declare -a TESTS_TO_EXCLUDE
if [ -f "${SKIP_LIST}" ]; then
    while IFS= read -r line
    do
        TESTS_TO_EXCLUDE+=("${line}")
    done < "${SKIP_LIST}"
fi

echo "..."
for EXCLUDE in "${TESTS_TO_EXCLUDE[@]}"; do
    for JJ in "${!TESTS_FOUND[@]}"; do
        if [[ ${TESTS_FOUND[$JJ]} = ${EXCLUDE} ]]; then
            echo "excluding: ${EXCLUDE}"
            unset 'TESTS_FOUND[$JJ]'
        fi
    done
done

echo "..."
echo "planning to run:"
for TEST in "${TESTS_FOUND[@]}"
do
    echo "${TEST}"
done
echo "..."

# download required artifacts for testing
# (cd ${TEST_DIR} && sudo bash ../.buildkite/download-images.sh)

# run selected tests
SUCCESS=0
CC_PYTEST_FLAGS="--cov=${SRC_DIR} --cov=${TEST_DIR} --cov-report=html:cc-vllm-html --cov-append"
for TEST in "${TESTS_FOUND[@]}"
do
    LOCAL_SUCCESS=0
    RESULT_XML=$(echo ${TEST} | sed -e "s/${TEST_DIR}/${RESULTS_DIR}/" | sed -e "s/.py/.xml/")

    # this is a bit messy and brittle, but certain tests
    # need to be run with specific options
    if [[ "${TEST}" == *"kernels"* || "${TEST}" == *"samplers"* ]]; then
        CUDA_VISIBLE_DEVICES=0,1 pytest ${CC_PYTEST_FLAGS} --junitxml=${RESULT_XML} ${TEST} || LOCAL_SUCCESS=$?
    elif [[ "${TEST}" == *"distributed/test_same_node"* ]]; then
        VLLM_TEST_SAME_HOST=1 torchrun --nproc-per-node=4 ${TEST} || LOCAL_SUCCESS=$?
    elif [[ "${TEST}" == *"distributed"* ]]; then
        CUDA_VISIBLE_DEVICES=0,1 pytest ${CC_PYTEST_FLAGS} --junitxml=${RESULT_XML} ${TEST} || LOCAL_SUCCESS=$?
    elif [[ "${TEST}" == *"test_models_logprobs"* ]]; then
        pytest --forked ${CC_PYTEST_FLAGS} --junitxml=${RESULT_XML} ${TEST} || LOCAL_SUCCESS=$?
    elif [[ "${TEST}" == *"basic_correctness/test_preemption"* ]]; then
        VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT=1 pytest ${CC_PYTEST_FLAGS} --junitxml=${RESULT_XML} ${TEST} || LOCAL_SUCCESS=$?
    else
        pytest ${CC_PYTEST_FLAGS} --junitxml=${RESULT_XML} ${TEST} || LOCAL_SUCCESS=$?
    fi

    SUCCESS=$((SUCCESS + LOCAL_SUCCESS))

done

if [ "${SUCCESS}" -eq "0" ]; then
    exit 0
else
    exit 1
fi
