#!/bin/bash -e

# simple helper script to manage concurrency while running tests

usage() {
    echo "Usage: ${0} <options>"
    echo
    echo "  -t    - test directory, i.e. location of *.py test files. (default 'tests/')"
    echo "  -r    - desired results base directory. xml results will mirror provided tests directory structure. (default 'test-results/')"
    echo "  -h    - this list of options"
    echo
    echo "note: all paths are relative to 'nm-vllm' root"
    echo
    exit 1
}

TEST_DIR=tests
RESULTS_DIR=test-results

while getopts "ht:r:" OPT; do
    case "${OPT}" in
	h)
	    usage
	    ;;
	t)
	    TEST_DIR="${OPTARG}"
	    ;;
    r)
        RESULTS_DIR="${OPTARG}"
        ;;
    esac
done

# check if variables are valid
if [ -z "${RESULTS_DIR}" ]; then
    echo "please set desired results base directory"
    usage
fi

if [ -z "${TEST_DIR}" ]; then
    echo "please set test directory"
    usage
fi

if [ ! -d "${TEST_DIR}" ]; then
    echo "specified test directory, '${TEST_DIR}' does not exist ..."
    usage
fi

# run tests serially
TESTS_DOT_PY=$(find ${TEST_DIR} -name "test*.py")
TESTS_FOUND=($TESTS_DOT_PY)

echo "found:"
echo "${TESTS_FOUND[@]}"

#    "tests/basic_correctness/test_basic_correctness.py"
#    "tests/lora/test_layers.py"
#    "tests/models/test_marlin.py"

TESTS_TO_EXCLUDE=(
    "tests/models/test_mistral.py"
    "tests/metrics/test_metrics.py"
    "tests/kernels/test_prefix_prefill.py"
    "tests/kernels/test_pos_encoding.py"
    "tests/kernels/test_activation.py"
    "tests/kernels/test_moe.py"
    "tests/kernels/test_layernorm.py"
    "tests/kernels/test_attention.py"
    "tests/kernels/test_cache.py"
    "tests/distributed/test_basic_distributed_correctness.py"
    "tests/distributed/test_custom_all_reduce.py"
    "tests/distributed/test_comm_ops.py"
    "tests/prefix_caching/test_prefix_caching.py"
    "tests/models/test_compressed_memory.py"
    "tests/models/test_compressed.py"
    "tests/models/test_models_logprobs.py"
    "tests/models/test_models.py"
    "tests/test_sampling_params.py"
    "tests/async_engine/test_async_llm_engine.py"
    "tests/async_engine/test_api_server.py"
    "tests/async_engine/test_chat_template.py"
    "tests/async_engine/test_request_tracker.py"
    "tests/samplers/test_beam_search.py"
    "tests/samplers/test_logprobs.py"
    "tests/samplers/test_seeded_generate.py"
    "tests/samplers/test_rejection_sampler.py"
    "tests/samplers/test_sampler.py"
    "tests/entrypoints/test_guided_processors.py"
    "tests/entrypoints/test_openai_server.py"
    "tests/lora/test_llama.py"
    "tests/lora/test_utils.py"
    "tests/lora/test_tokenizer.py"
    "tests/lora/test_gemma.py"
    "tests/lora/test_lora_manager.py"
    "tests/lora/test_worker.py"
    "tests/lora/test_mixtral.py"
    "tests/lora/test_punica.py"
    "tests/lora/test_lora.py"
    "tests/worker/spec_decode/test_multi_step_worker.py"
    "tests/worker/test_model_runner.py"
    "tests/engine/test_detokenize.py"
    "tests/test_cache_block_hashing.py"
    "tests/test_regression.py")

echo "excluded:"
echo "${TESTS_TO_EXCLUDE[@]}"

TESTS_TO_RUN=()
for EXCLUDE in "${TESTS_TO_EXCLUDE[@]}"; do
    for JJ in "${!TESTS_FOUND[@]}"; do
        if [[ ${TESTS_FOUND[$JJ]} = ${EXCLUDE} ]]; then
            echo "excluding: ${EXCLUDE}"
            unset 'TESTS_FOUND[$JJ]'
        fi
    done
done

echo "running:"
echo "${TESTS_FOUND[@]}"

# exit 0

# TESTS_TO_RUN=($TESTS_DOT_PY)
SUCCESS=0
for TEST in "${TESTS_FOUND[@]}"
do
    LOCAL_SUCCESS=0
    RESULT_XML=$(echo ${TEST} | sed -e "s/${TEST_DIR}/${RESULTS_DIR}/" | sed -e "s/.py/.xml/")

    echo "planning to generate ... .coverage-$(basename ${TEST})"

    # this is a bit messy and brittle, but certain tests
    # need to be run with specific options
    if [[ "${TEST}" == *"kernels/test_pos_encoding"* ]]; then
        CUDA_VISIBLE_DEVICES=0,1 coverage run --data-file=.coverage-$(basename ${TEST}) -m pytest --forked --junitxml=${RESULT_XML} ${TEST} || LOCAL_SUCCESS=$?
    elif [[ "${TEST}" == *"kernels"* ]]; then
        CUDA_VISIBLE_DEVICES=0,1 coverage run --data-file=.coverage-$(basename ${TEST}) -m pytest --junitxml=${RESULT_XML} ${TEST} || LOCAL_SUCCESS=$?
    elif [[ "${TEST}" == *"samplers"* ]]; then
        CUDA_VISIBLE_DEVICES=0,1 coverage run --data-file=.coverage-$(basename ${TEST}) -m pytest --junitxml=${RESULT_XML} ${TEST} || LOCAL_SUCCESS=$?
    elif [[ "${TEST}" == *"distributed"* ]]; then
        coverage run --data-file=.coverage-$(basename ${TEST}) -m pytest --forked --junitxml=${RESULT_XML} ${TEST} || LOCAL_SUCCESS=$?
    elif [[ "${TEST}" == *"models_logprobs"* ]]; then
        coverage run --data-file=.coverage-$(basename ${TEST}) -m pytest --forked --junitxml=${RESULT_XML} ${TEST} || LOCAL_SUCCESS=$?
    else
        coverage run --data-file=.coverage-$(basename ${TEST}) -m pytest --junitxml=${RESULT_XML} ${TEST} || LOCAL_SUCCESS=$?
    fi

    SUCCESS=$((SUCCESS + LOCAL_SUCCESS))

done

if [ "${SUCCESS}" -eq "0" ]; then
    exit 0
else
    exit 1
fi
