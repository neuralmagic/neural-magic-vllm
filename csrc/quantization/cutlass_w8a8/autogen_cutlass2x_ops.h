#pragma once
#define CUTLASS2X_DEFS\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8);\


void autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x128x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x64x128_16x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_16x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemm_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x64_16x64x64_16x8x32_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_16x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);
