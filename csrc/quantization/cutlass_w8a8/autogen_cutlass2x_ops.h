#pragma once
#define CUTLASS2X_DEFS\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x32x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x32x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x32x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x32x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x32x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x32x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x32x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x32x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x32x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x32x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x32x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x32x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x64_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x64_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x64_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x64_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x64x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x64x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x64x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x64x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x64x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x64x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x64x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x64x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x32x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x32x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x32x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x32x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x32x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x32x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x32x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x32x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x128_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x128_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x128_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x128_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x64x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x64x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x64x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x64x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x32x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x32x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x32x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x32x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x64x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x64x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x64x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x64x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x64x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x64x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x64x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x64x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\
 ops.def("autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(Tensor! out, Tensor a, Tensor b,Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()", &autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8); \
 ops.impl("autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8", torch::kCUDA, &autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8);\


void autogen_cutlass2x_scaled_mm_sm75_128x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x64x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x64x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x64x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x64x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x64x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x32x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x64x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x128_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x64x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x64x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x64x64_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x32x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x64x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x32x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x32x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x32x64_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x64_128x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x64x128_128x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x32x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x64x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x64x128_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x32x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x64_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x64x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x128_32x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x128_128x128x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_32x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x64x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x128x128_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x32x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_128x64x128_128x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x128_64x32x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);

void autogen_cutlass2x_scaled_mm_sm75_64x128x64_32x64x64_8x8x16_ThreadblockSwizzleStreamK_kGemmSplitKParallel_2_OpMultiplyAdd_i8(torch::Tensor &out, torch::Tensor const &a,
                torch::Tensor const &b,
                torch::Tensor const &a_scales,
                torch::Tensor const &b_scales,
                c10::optional<torch::Tensor> const& bias);
